# -*- coding: utf-8 -*-
"""Kmeans_Retail_Sachin_Malik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fCDsvYx1bEfMKOhabk672CGcXyjcxNKT
"""

# Cell 1: Imports & display options
import os
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
import matplotlib.pyplot as plt

# nicer display in notebooks
pd.set_option("display.max_columns", 200)
pd.set_option("display.width", 200)

# Cell 2: File path and load
# Update this path if your file is elsewhere.
user_path = r"/content/Online Retail.csv"

if os.path.exists(user_path):
    file_path = user_path
elif os.path.exists(alt_path):
    file_path = alt_path
else:
    raise FileNotFoundError(f"File not found at either:\n  {user_path}\n  {alt_path}\nUpdate the path and re-run.")

print("Loading from:", file_path)
df = pd.read_csv(file_path, encoding="ISO-8859-1")   # encoding often required for this dataset
print("Raw shape:", df.shape)

# Cell 3: Explore basic info
print("Columns:", df.columns.tolist())
print("\nHEAD:")
display(df.head())

print("\nINFO:")
display(df.info())

print("\nNUMERIC DESCRIBE:")
display(df.describe(include=[np.number]).T)

# Missing values summary
missing_count = df.isnull().sum().sort_values(ascending=False)
missing_pct = (missing_count / len(df) * 100).round(2)
miss_df = pd.concat([missing_count, missing_pct], axis=1)
miss_df.columns = ["missing_count", "missing_pct"]
print("\nMissing values (non-zero only):")
display(miss_df[miss_df['missing_count'] > 0])

# Cell 4: Clean column names and convert types
data = df.copy()
# strip whitespace in column names
data.columns = [c.strip() for c in data.columns]

# Convert InvoiceDate -> datetime (coerce bad values)
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], errors='coerce')

# Convert numeric columns
data['Quantity'] = pd.to_numeric(data['Quantity'], errors='coerce')
data['UnitPrice'] = pd.to_numeric(data['UnitPrice'], errors='coerce')

print("After conversions - nulls in InvoiceDate/Quantity/UnitPrice:")
display(data[['InvoiceDate','Quantity','UnitPrice']].isnull().sum())

# Cell 5: Remove rows missing essential information and duplicates
# Important fields: InvoiceNo, StockCode, InvoiceDate
before = data.shape[0]
data = data.dropna(subset=['InvoiceNo','StockCode','InvoiceDate'])
after_drop = data.shape[0]
print(f"Dropped {before - after_drop} rows missing essential fields.")

# Fill Description with placeholder (optional)
if 'Description' in data.columns:
    data['Description'] = data['Description'].fillna('Unknown')

# Remove exact duplicate rows
before_dup = data.shape[0]
data = data.drop_duplicates()
after_dup = data.shape[0]
print(f"Removed {before_dup - after_dup} exact duplicate rows. New shape: {data.shape}")

# Cell 6: Create Sales column and handle negative/return values
data['Sales'] = data['Quantity'] * data['UnitPrice']

# Flag rows that are returns or negative
data['is_return'] = (data['Quantity'] < 0) | (data['UnitPrice'] < 0) | (data['Sales'] < 0)

print("Total rows:", len(data))
print("Return rows flagged:", data['is_return'].sum(), "(", round(data['is_return'].sum()/len(data)*100,2), "% )")

# Split datasets for easier downstream analysis
returns_df = data[data['is_return']].copy()
cleaned_df = data[~data['is_return']].copy()

print("Cleaned (non-returns) shape:", cleaned_df.shape)
print("Returns shape:", returns_df.shape)

# Optionally, remove zero-quantity or zero-price rows from cleaned set (often meaningless)
cleaned_df = cleaned_df[(cleaned_df['Quantity'] > 0) & (cleaned_df['UnitPrice'] >= 0)]
print("Cleaned shape after enforcing Quantity>0 and UnitPrice>=0:", cleaned_df.shape)

# Cell 7: Handling missing CustomerID (choose one approach)
print("Missing CustomerID in cleaned set:", cleaned_df['CustomerID'].isnull().sum())

# Option A: Keep them as NaN (useful if you want to treat anonymous customers separately)
# Option B: Drop rows missing CustomerID (useful for customer-level aggregations)
# Option C: Fill with a placeholder like 0 or -1 (not recommended for IDs unless you're explicit)

# Example: create a flag for missing CustomerID and keep rows
cleaned_df['CustomerID_missing'] = cleaned_df['CustomerID'].isnull()

# If you prefer to drop them uncomment the next line:
cleaned_df = cleaned_df.dropna(subset=['CustomerID'])

# Cell 8: Feature engineering (datetime pieces, invoice aggregations)
df_fe = cleaned_df.copy()

# Datetime features
df_fe['InvoiceYear']  = df_fe['InvoiceDate'].dt.year
df_fe['InvoiceMonth'] = df_fe['InvoiceDate'].dt.month
df_fe['InvoiceDay']   = df_fe['InvoiceDate'].dt.day
df_fe['InvoiceHour']  = df_fe['InvoiceDate'].dt.hour
df_fe['InvoiceWeekday']= df_fe['InvoiceDate'].dt.weekday  # Monday=0

# Invoice-level aggregates
invoice_qty = df_fe.groupby('InvoiceNo')['Quantity'].sum().rename('TotalItemsPerInvoice')
invoice_sales = df_fe.groupby('InvoiceNo')['Sales'].sum().rename('TotalSalesPerInvoice')
invoice_unique_products = df_fe.groupby('InvoiceNo')['StockCode'].nunique().rename('UniqueProductsPerInvoice')

# Merge back to row-level (so each row has invoice totals)
df_fe = df_fe.merge(invoice_qty, left_on='InvoiceNo', right_index=True, how='left')
df_fe = df_fe.merge(invoice_sales, left_on='InvoiceNo', right_index=True, how='left')
df_fe = df_fe.merge(invoice_unique_products, left_on='InvoiceNo', right_index=True, how='left')

# Convert CustomerID to integer dtype where possible (pandas nullable Int)
df_fe['CustomerID'] = pd.to_numeric(df_fe['CustomerID'], errors='coerce').astype('Int64')

print("Feature engineered columns added. Sample:")
display(df_fe.head())

# Cell 9: Encode categorical variables
df_enc = df_fe.copy()

# Country: reduce to top N categories and mark rest as 'Other' then one-hot encode
top_n = 10
top_countries = df_enc['Country'].value_counts().head(top_n).index.tolist()
df_enc['Country_reduced'] = df_enc['Country'].where(df_enc['Country'].isin(top_countries), other='Other')
country_dummies = pd.get_dummies(df_enc['Country_reduced'], prefix='Country', drop_first=False)
df_enc = pd.concat([df_enc, country_dummies], axis=1)

# StockCode: label encode (useful as an ID; for modeling prefer frequency/target encoding)
le_sc = LabelEncoder()
df_enc['StockCode_le'] = le_sc.fit_transform(df_enc['StockCode'].astype(str))

print("Encoded country (one-hot) and label-encoded StockCode. Columns example:")
print([c for c in df_enc.columns if c.startswith('Country_')][:10] + ['StockCode_le'])
display(df_enc.head())

# Cell 10: Scaling numeric features (standard + min-max)
num_cols = ['Quantity','UnitPrice','Sales','TotalItemsPerInvoice','TotalSalesPerInvoice','UniqueProductsPerInvoice']
# ensure no NaNs in scaling
df_enc[num_cols] = df_enc[num_cols].fillna(0)

scaler_std = StandardScaler()
scaler_mm  = MinMaxScaler()

# Standard scaled
df_enc[[c + '_std' for c in num_cols]] = scaler_std.fit_transform(df_enc[num_cols])
# Min-Max scaled
df_enc[[c + '_mm' for c in num_cols]] = scaler_mm.fit_transform(df_enc[num_cols])

print("Added scaled columns. Sample:")
display(df_enc[[*num_cols, *(c + '_std' for c in num_cols), *(c + '_mm' for c in num_cols)]].head())

# Cell 11: Correlation (helpful for quick feature selection)
numeric_for_corr = df_enc.select_dtypes(include=[np.number]).copy()
corr = numeric_for_corr.corr()
sales_corr = corr['Sales'].drop('Sales').abs().sort_values(ascending=False)
print("Top correlations with Sales:")
display(sales_corr.head(20))

# Cell 12: Save outputs
out_dir = os.path.join(os.getcwd(), "cleaned_output")
os.makedirs(out_dir, exist_ok=True)

cleaned_out = os.path.join(out_dir, "online_retail_cleaned.csv")
returns_out = os.path.join(out_dir, "online_retail_returns.csv")

df_enc.to_csv(cleaned_out, index=False)
returns_df.to_csv(returns_out, index=False)

print("Saved cleaned dataset to:", cleaned_out)
print("Saved returns dataset to:", returns_out)

# Cell 13: A few basic plots (optional)
import matplotlib.pyplot as plt

# 1) Sales over time (by day)
daily = df_enc.set_index('InvoiceDate').resample('D')['Sales'].sum().fillna(0)
plt.figure(figsize=(12,4))
plt.plot(daily.index, daily.values)
plt.title("Daily Sales")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.show()

# 2) Top 10 products by total sales
top_products = df_enc.groupby('Description')['Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10,5))
top_products[::-1].plot(kind='barh')
plt.title("Top 10 Products by Sales")
plt.xlabel("Sales")
plt.show()

df.columns

df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')

df = df.dropna(subset=['InvoiceDate'])

import pandas as pd

# Fix InvoiceDate conversion
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')

# Drop rows where InvoiceDate is missing
df = df.dropna(subset=['InvoiceDate'])

# Create TotalPrice if not created
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Reference date (1 day after max invoice date)
ref_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# RFM calculation
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (ref_date - x.max()).days,  # Recency
    'InvoiceNo': 'count',                                # Frequency
    'TotalPrice': 'sum'                                  # Monetary
})

rfm.columns = ['Recency', 'Frequency', 'Monetary']
rfm.head()

from sklearn.preprocessing import StandardScaler

features = ['Recency', 'Frequency', 'Monetary']
X = rfm[features]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []

for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
rfm['Cluster'] = kmeans.fit_predict(X_scaled)

rfm.head()

rfm.groupby('Cluster').mean().round(2)

rfm['Cluster'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Cluster', palette='viridis')
plt.title('Customer Segments: Recency vs Monetary')
plt.show()

sns.scatterplot(data=rfm, x='Frequency', y='Monetary', hue='Cluster', palette='viridis')
plt.title('Customer Segments: Frequency vs Monetary')
plt.show()

rfm.groupby('Cluster').mean().round(2)

rfm_plot = rfm.reset_index()

plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm_plot, x="Recency", y="Frequency", hue="Cluster", palette="Set2")
plt.title("Customer Segments: Recency vs Frequency")
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm_plot, x="Frequency", y="Monetary", hue="Cluster", palette="Set2")
plt.title("Customer Segments: Frequency vs Monetary")
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm_plot, x="Recency", y="Monetary", hue="Cluster", palette="Set2")
plt.title("Customer Segments: Recency vs Monetary")
plt.show()

centroids = pd.DataFrame(kmeans.cluster_centers_, columns=["Recency", "Frequency", "Monetary"])

plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm_plot, x="Recency", y="Monetary", hue="Cluster", palette="Set2")
sns.scatterplot(data=centroids, x="Recency", y="Monetary", color="black", s=300, marker="X", label="Centroids")
plt.title("Clusters with Centroids")
plt.legend()
plt.show()

cluster_summary = rfm.groupby("Cluster").agg({
    "Recency": ["mean", "median", "std"],
    "Frequency": ["mean", "median", "std"],
    "Monetary": ["mean", "median", "std"]
})

cluster_summary

plt.figure(figsize=(15,5))

plt.subplot(1,3,1)
sns.boxplot(data=rfm_plot, x="Cluster", y="Recency")
plt.title("Recency by Cluster")

plt.subplot(1,3,2)
sns.boxplot(data=rfm_plot, x="Cluster", y="Frequency")
plt.title("Frequency by Cluster")

plt.subplot(1,3,3)
sns.boxplot(data=rfm_plot, x="Cluster", y="Monetary")
plt.title("Monetary by Cluster")

plt.tight_layout()
plt.show()

cleaned_df_clustered = cleaned_df.merge(
    rfm_plot[['CustomerID', 'Cluster']],
    on='CustomerID',
    how='left'
)

cleaned_df_clustered.head()

# Add cluster labels to RFM dataframe
rfm_kmeans = rfm.copy()
rfm_kmeans['Cluster'] = kmeans.labels_

rfm_kmeans.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Scatter Plot: Recency vs Frequency
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=rfm_kmeans,
    x='Recency',
    y='Frequency',
    hue='Cluster',
    palette='tab10'
)
plt.title("Recency vs Frequency by Cluster")
plt.show()

# Scatter Plot: Frequency vs Monetary
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=rfm_kmeans,
    x='Frequency',
    y='Monetary',
    hue='Cluster',
    palette='tab10'
)
plt.title("Frequency vs Monetary by Cluster")
plt.show()

# Scatter Plot: Recency vs Monetary
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=rfm_kmeans,
    x='Recency',
    y='Monetary',
    hue='Cluster',
    palette='tab10'
)
plt.title("Recency vs Monetary by Cluster")
plt.show()

# Get scaled centroids and inverse-transform
centroids_scaled = kmeans.cluster_centers_
centroids = scaler.inverse_transform(centroids_scaled)

centroid_df = pd.DataFrame(centroids, columns=['Recency', 'Frequency', 'Monetary'])
centroid_df['Cluster'] = range(len(centroid_df))
centroid_df

plt.figure(figsize=(8,6))
sns.scatterplot(
    data=rfm_kmeans,
    x='Recency',
    y='Frequency',
    hue='Cluster',
    palette='tab10'
)

# Plot centroids
plt.scatter(
    centroid_df['Recency'],
    centroid_df['Frequency'],
    s=300,
    marker='X',
    color='black',
    label='Centroid'
)

plt.title("Clusters with Centroids (Recency vs Frequency)")
plt.legend()
plt.show()

cluster_summary = rfm_kmeans.groupby('Cluster').agg({
    'Recency': ['mean', 'median', 'std'],
    'Frequency': ['mean', 'median', 'std'],
    'Monetary': ['mean', 'median', 'std']
})

cluster_summary

for c in sorted(rfm_kmeans['Cluster'].unique()):
    print(f"\nCluster {c} Profile:")
    print(rfm_kmeans[rfm_kmeans['Cluster'] == c].describe()[['Recency','Frequency','Monetary']])

