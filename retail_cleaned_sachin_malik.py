# -*- coding: utf-8 -*-
"""Retail_cleaned_sachin_malik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VMB46FxRTJ-JmunLNOVcKPAcFZ2dheiU
"""

# Cell 1: Imports & display options
import os
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
import matplotlib.pyplot as plt

# nicer display in notebooks
pd.set_option("display.max_columns", 200)
pd.set_option("display.width", 200)

# Cell 2: File path and load
# Update this path if your file is elsewhere.
user_path = r"/content/Online Retail.csv"

if os.path.exists(user_path):
    file_path = user_path
elif os.path.exists(alt_path):
    file_path = alt_path
else:
    raise FileNotFoundError(f"File not found at either:\n  {user_path}\n  {alt_path}\nUpdate the path and re-run.")

print("Loading from:", file_path)
df = pd.read_csv(file_path, encoding="ISO-8859-1")   # encoding often required for this dataset
print("Raw shape:", df.shape)

# Cell 3: Explore basic info
print("Columns:", df.columns.tolist())
print("\nHEAD:")
display(df.head())

print("\nINFO:")
display(df.info())

print("\nNUMERIC DESCRIBE:")
display(df.describe(include=[np.number]).T)

# Missing values summary
missing_count = df.isnull().sum().sort_values(ascending=False)
missing_pct = (missing_count / len(df) * 100).round(2)
miss_df = pd.concat([missing_count, missing_pct], axis=1)
miss_df.columns = ["missing_count", "missing_pct"]
print("\nMissing values (non-zero only):")
display(miss_df[miss_df['missing_count'] > 0])

# Cell 4: Clean column names and convert types
data = df.copy()
# strip whitespace in column names
data.columns = [c.strip() for c in data.columns]

# Convert InvoiceDate -> datetime (coerce bad values)
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], errors='coerce')

# Convert numeric columns
data['Quantity'] = pd.to_numeric(data['Quantity'], errors='coerce')
data['UnitPrice'] = pd.to_numeric(data['UnitPrice'], errors='coerce')

print("After conversions - nulls in InvoiceDate/Quantity/UnitPrice:")
display(data[['InvoiceDate','Quantity','UnitPrice']].isnull().sum())

# Cell 5: Remove rows missing essential information and duplicates
# Important fields: InvoiceNo, StockCode, InvoiceDate
before = data.shape[0]
data = data.dropna(subset=['InvoiceNo','StockCode','InvoiceDate'])
after_drop = data.shape[0]
print(f"Dropped {before - after_drop} rows missing essential fields.")

# Fill Description with placeholder (optional)
if 'Description' in data.columns:
    data['Description'] = data['Description'].fillna('Unknown')

# Remove exact duplicate rows
before_dup = data.shape[0]
data = data.drop_duplicates()
after_dup = data.shape[0]
print(f"Removed {before_dup - after_dup} exact duplicate rows. New shape: {data.shape}")

# Cell 6: Create Sales column and handle negative/return values
data['Sales'] = data['Quantity'] * data['UnitPrice']

# Flag rows that are returns or negative
data['is_return'] = (data['Quantity'] < 0) | (data['UnitPrice'] < 0) | (data['Sales'] < 0)

print("Total rows:", len(data))
print("Return rows flagged:", data['is_return'].sum(), "(", round(data['is_return'].sum()/len(data)*100,2), "% )")

# Split datasets for easier downstream analysis
returns_df = data[data['is_return']].copy()
cleaned_df = data[~data['is_return']].copy()

print("Cleaned (non-returns) shape:", cleaned_df.shape)
print("Returns shape:", returns_df.shape)

# Optionally, remove zero-quantity or zero-price rows from cleaned set (often meaningless)
cleaned_df = cleaned_df[(cleaned_df['Quantity'] > 0) & (cleaned_df['UnitPrice'] >= 0)]
print("Cleaned shape after enforcing Quantity>0 and UnitPrice>=0:", cleaned_df.shape)

# Cell 7: Handling missing CustomerID (choose one approach)
print("Missing CustomerID in cleaned set:", cleaned_df['CustomerID'].isnull().sum())

# Option A: Keep them as NaN (useful if you want to treat anonymous customers separately)
# Option B: Drop rows missing CustomerID (useful for customer-level aggregations)
# Option C: Fill with a placeholder like 0 or -1 (not recommended for IDs unless you're explicit)

# Example: create a flag for missing CustomerID and keep rows
cleaned_df['CustomerID_missing'] = cleaned_df['CustomerID'].isnull()

# If you prefer to drop them uncomment the next line:
cleaned_df = cleaned_df.dropna(subset=['CustomerID'])

# Cell 8: Feature engineering (datetime pieces, invoice aggregations)
df_fe = cleaned_df.copy()

# Datetime features
df_fe['InvoiceYear']  = df_fe['InvoiceDate'].dt.year
df_fe['InvoiceMonth'] = df_fe['InvoiceDate'].dt.month
df_fe['InvoiceDay']   = df_fe['InvoiceDate'].dt.day
df_fe['InvoiceHour']  = df_fe['InvoiceDate'].dt.hour
df_fe['InvoiceWeekday']= df_fe['InvoiceDate'].dt.weekday  # Monday=0

# Invoice-level aggregates
invoice_qty = df_fe.groupby('InvoiceNo')['Quantity'].sum().rename('TotalItemsPerInvoice')
invoice_sales = df_fe.groupby('InvoiceNo')['Sales'].sum().rename('TotalSalesPerInvoice')
invoice_unique_products = df_fe.groupby('InvoiceNo')['StockCode'].nunique().rename('UniqueProductsPerInvoice')

# Merge back to row-level (so each row has invoice totals)
df_fe = df_fe.merge(invoice_qty, left_on='InvoiceNo', right_index=True, how='left')
df_fe = df_fe.merge(invoice_sales, left_on='InvoiceNo', right_index=True, how='left')
df_fe = df_fe.merge(invoice_unique_products, left_on='InvoiceNo', right_index=True, how='left')

# Convert CustomerID to integer dtype where possible (pandas nullable Int)
df_fe['CustomerID'] = pd.to_numeric(df_fe['CustomerID'], errors='coerce').astype('Int64')

print("Feature engineered columns added. Sample:")
display(df_fe.head())

# Cell 9: Encode categorical variables
df_enc = df_fe.copy()

# Country: reduce to top N categories and mark rest as 'Other' then one-hot encode
top_n = 10
top_countries = df_enc['Country'].value_counts().head(top_n).index.tolist()
df_enc['Country_reduced'] = df_enc['Country'].where(df_enc['Country'].isin(top_countries), other='Other')
country_dummies = pd.get_dummies(df_enc['Country_reduced'], prefix='Country', drop_first=False)
df_enc = pd.concat([df_enc, country_dummies], axis=1)

# StockCode: label encode (useful as an ID; for modeling prefer frequency/target encoding)
le_sc = LabelEncoder()
df_enc['StockCode_le'] = le_sc.fit_transform(df_enc['StockCode'].astype(str))

print("Encoded country (one-hot) and label-encoded StockCode. Columns example:")
print([c for c in df_enc.columns if c.startswith('Country_')][:10] + ['StockCode_le'])
display(df_enc.head())

# Cell 10: Scaling numeric features (standard + min-max)
num_cols = ['Quantity','UnitPrice','Sales','TotalItemsPerInvoice','TotalSalesPerInvoice','UniqueProductsPerInvoice']
# ensure no NaNs in scaling
df_enc[num_cols] = df_enc[num_cols].fillna(0)

scaler_std = StandardScaler()
scaler_mm  = MinMaxScaler()

# Standard scaled
df_enc[[c + '_std' for c in num_cols]] = scaler_std.fit_transform(df_enc[num_cols])
# Min-Max scaled
df_enc[[c + '_mm' for c in num_cols]] = scaler_mm.fit_transform(df_enc[num_cols])

print("Added scaled columns. Sample:")
display(df_enc[[*num_cols, *(c + '_std' for c in num_cols), *(c + '_mm' for c in num_cols)]].head())

# Cell 11: Correlation (helpful for quick feature selection)
numeric_for_corr = df_enc.select_dtypes(include=[np.number]).copy()
corr = numeric_for_corr.corr()
sales_corr = corr['Sales'].drop('Sales').abs().sort_values(ascending=False)
print("Top correlations with Sales:")
display(sales_corr.head(20))

# Cell 12: Save outputs
out_dir = os.path.join(os.getcwd(), "cleaned_output")
os.makedirs(out_dir, exist_ok=True)

cleaned_out = os.path.join(out_dir, "online_retail_cleaned.csv")
returns_out = os.path.join(out_dir, "online_retail_returns.csv")

df_enc.to_csv(cleaned_out, index=False)
returns_df.to_csv(returns_out, index=False)

print("Saved cleaned dataset to:", cleaned_out)
print("Saved returns dataset to:", returns_out)

# Cell 13: A few basic plots (optional)
import matplotlib.pyplot as plt

# 1) Sales over time (by day)
daily = df_enc.set_index('InvoiceDate').resample('D')['Sales'].sum().fillna(0)
plt.figure(figsize=(12,4))
plt.plot(daily.index, daily.values)
plt.title("Daily Sales")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.show()

# 2) Top 10 products by total sales
top_products = df_enc.groupby('Description')['Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10,5))
top_products[::-1].plot(kind='barh')
plt.title("Top 10 Products by Sales")
plt.xlabel("Sales")
plt.show()

